{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ikDabHwKgbaz"
   },
   "source": [
    "# **EXPERIMENT-2** : **RNN + FastText and Bi-LSTM + FastText**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oo8kUanPSxt1"
   },
   "source": [
    "### **Objective: Compare RNN and Bi-LSTM models trained using FastText embeddings and evaluate performance differences.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade --force-reinstall numpy==1.26.4 scipy==1.13.1 datasets gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets #for loading PLOD-CW-25\n",
    "!pip install gensim #for loading FastText embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Loading the PLOD-CW-25 dataset from Hugging Face\n",
    "dataset = load_dataset(\"surrey-nlp/PLOD-CW-25\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def set_seed(seed: int, verbose: bool = False) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  # For multi-GPU setups\n",
    "\n",
    "    # Ensure deterministic behavior (at the cost of performance)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[INFO] Seed set to {seed} (deterministic: True, benchmark: False)\")\n",
    "\n",
    "# Usage\n",
    "set_seed(24, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zwbfdjkTsQni"
   },
   "source": [
    "# A. Preprocessing Tokens, Labels and POS Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Buyo_2XpdgPc"
   },
   "source": [
    "### 1. Token Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting tokens and converting to lowercase properly\n",
    "tokens_train = [[token.lower() for token in ex['tokens']] for ex in dataset['train']]\n",
    "tokens_val   = [[token.lower() for token in ex['tokens']] for ex in dataset['validation']]\n",
    "tokens_test  = [[token.lower() for token in ex['tokens']] for ex in dataset['test']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kDjcCI9thXen"
   },
   "source": [
    "### 2. NER Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting all unique NER tags from the training set\n",
    "unique_ner_tags = set(tag for ex in dataset['train'] for tag in ex['ner_tags'])\n",
    "ner2id = {tag: idx for idx, tag in enumerate(sorted(unique_ner_tags))}\n",
    "\n",
    "ner_tag_list = sorted(list(unique_ner_tags))\n",
    "print(\"Unique NER tags:\", ner_tag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_ner_tags(example):\n",
    "    example['ner_ids'] = [ner2id[tag] for tag in example['ner_tags']]\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset = dataset.map(encode_ner_tags, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoded_dataset['train'].column_names)\n",
    "print(encoded_dataset['validation'].column_names)\n",
    "print(encoded_dataset['test'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking a sample to confirm\n",
    "for i in range(3):\n",
    "    print(f\"\\nExample {i+1}\")\n",
    "    print(\"Tokens:     \", encoded_dataset['train'][i]['tokens'])\n",
    "    print(\"NER tags:   \", encoded_dataset['train'][i]['ner_tags'])\n",
    "    print(\"NER IDs:    \", encoded_dataset['train'][i]['ner_ids'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract for inspection\n",
    "ner_train = [sentence['ner_tags'] for sentence in dataset['train']]\n",
    "ner_val   = [sentence['ner_tags'] for sentence in dataset['validation']]\n",
    "ner_test  = [sentence['ner_tags'] for sentence in dataset['test']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoded_dataset['train'].column_names)\n",
    "print(encoded_dataset['validation'].column_names)\n",
    "print(encoded_dataset['test'].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kb2AhO4lYAl2"
   },
   "source": [
    "### 3. POS Tag Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting POS tags\n",
    "unique_pos_tags = set()\n",
    "for example in dataset['train']:\n",
    "    unique_pos_tags.update(example['pos_tags'])\n",
    "\n",
    "pos_tag_list = sorted(list(unique_pos_tags))\n",
    "print(\" Unique POS tags:\")\n",
    "print(pos_tag_list)\n",
    "print(f\"Total POS tag types: {len(pos_tag_list)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating mappings\n",
    "pos2id = {pos: idx for idx, pos in enumerate(pos_tag_list)}\n",
    "id2pos = {idx: pos for pos, idx in pos2id.items()}\n",
    "\n",
    "# Printing sample of the mapping\n",
    "print(\"\\nPOS to ID mapping (sample):\")\n",
    "for i, (k, v) in enumerate(pos2id.items()):\n",
    "    print(f\"{k}: {v}\")\n",
    "    if i == 10: break  # just show first 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoded_dataset['train'].column_names)\n",
    "print(encoded_dataset['validation'].column_names)\n",
    "print(encoded_dataset['test'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding POS tags in each example\n",
    "def encode_pos_tags(example):\n",
    "    example['pos_ids'] = [pos2id[pos] for pos in example['pos_tags']]\n",
    "    return example\n",
    "\n",
    "# First encode NER tags\n",
    "encoded_dataset = dataset.map(encode_ner_tags)\n",
    "encoded_dataset = encoded_dataset.map(encode_pos_tags)\n",
    "\n",
    "print(\"\\n Sample POS tags and encoded IDs:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(\"POS Tags:\", encoded_dataset['train'][i]['pos_tags'])\n",
    "    print(\"POS IDs: \", encoded_dataset['train'][i]['pos_ids'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoded_dataset['train'].column_names)\n",
    "print(encoded_dataset['validation'].column_names)\n",
    "print(encoded_dataset['test'].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ChVGJHnEd61B"
   },
   "source": [
    "### 4. Checking alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in ['train', 'validation', 'test']:\n",
    "    for example in encoded_dataset[split]:\n",
    "        assert len(example['tokens']) == len(example['pos_ids']) == len(example['ner_ids']), \\\n",
    "            f\"Alignment issue in {split} split! Example: {example}\"\n",
    "print(\" All sequences (tokens, POS tags, NER tags) are correctly aligned.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First three examples\n",
    "for i in range(3):\n",
    "    ex = encoded_dataset['train'][i]\n",
    "    print(f\"\\n Example {i+1}\")\n",
    "    print(\"Tokens:   \", ex['tokens'])\n",
    "    print(\"POS Tags: \", ex['pos_tags'])\n",
    "    print(\"POS IDs:  \", ex['pos_ids'])\n",
    "    print(\"NER Tags: \", ex['ner_tags'])\n",
    "    print(\"NER IDs:  \", ex['ner_ids'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting for inspection:\n",
    "pos_train = [ex['pos_tags'] for ex in encoded_dataset['train']]\n",
    "pos_val   = [ex['pos_tags'] for ex in encoded_dataset['validation']]\n",
    "pos_test  = [ex['pos_tags'] for ex in encoded_dataset['test']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Scn2eaJydCb"
   },
   "source": [
    "### 5. Padding tokens, ner_tags, and pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting max length of any sequence in training set\n",
    "max_len = max(len(ex['tokens']) for ex in encoded_dataset['train'])\n",
    "print(f\" Max sequence length in training set: {max_len}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(seq, max_len, pad_value):\n",
    "    return seq + [pad_value] * (max_len - len(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_example(example):\n",
    "    example['pos_ids_padded'] = pad_sequence(example['pos_ids'], max_len, pad_value=0)\n",
    "    example['ner_ids_padded'] = pad_sequence(example['ner_ids'], max_len, pad_value=-100)  # -100 is often used for ignored loss positions\n",
    "    return example\n",
    "\n",
    "padded_dataset = encoded_dataset.map(pad_example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    ex = padded_dataset['train'][i]\n",
    "    print(f\"\\n Example {i+1}\")\n",
    "    print(\"POS IDs (padded):\", ex['pos_ids_padded'])\n",
    "    print(\"NER IDs (padded):\", ex['ner_ids_padded'])\n",
    "    print(\"Original length:\", len(ex['pos_ids']), \"| Padded length:\", len(ex['pos_ids_padded']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgHiz6eFeOd0"
   },
   "source": [
    "# B. Word Embedding using FastText (Facebook Pre-trained Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6UCqKx1lyJat"
   },
   "source": [
    "## Download and load FastText Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
    "!gunzip cc.en.300.bin.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "fasttext_model = fasttext.load_model('cc.en.300.bin')\n",
    "\n",
    "print(fasttext_model.get_dimension())\n",
    "print(fasttext_model.get_words()[:10])  # Prints the first 10 words in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embedding vector for a word\n",
    "vector = fasttext_model.get_word_vector('biology')\n",
    "print(vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check nearest neighbors\n",
    "neighbors = fasttext_model.get_nearest_neighbors('biology', k=5)\n",
    "for score, word in neighbors:\n",
    "    print(f\"{word}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qKSx2xFD2Fgt"
   },
   "source": [
    "#### 1. Out-Of-Vocabulary (OOV) Tokens Coverage Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting all tokens from train, validation, and test splits\n",
    "token_lists = [ex['tokens'] for split in ['train'] for ex in padded_dataset[split]]\n",
    "\n",
    "# Flatten token lists and build a unique vocabulary set\n",
    "vocab = set(token for sent in token_lists for token in sent)\n",
    "\n",
    "print(f\"Vocab size: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of OOV tokens\n",
    "oov_tokens = [token for token in vocab if token not in fasttext_model]\n",
    "\n",
    "print(f\" Number of OOV tokens: {len(oov_tokens)}\")\n",
    "print(\"Sample OOV tokens:\", oov_tokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking if FastText Can Still Embed OOV Tokens\n",
    "can_embed = 0\n",
    "for token in oov_tokens:\n",
    "    try:\n",
    "        _ = fasttext_model[token]\n",
    "        can_embed += 1\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "print(f\"FastText can generate embeddings for {can_embed} / {len(oov_tokens)} OOV tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "moqRlKQub-j8"
   },
   "source": [
    "The coverage analysis of the dataset vocabulary against the FastText model revealed a total of 1,802 out-of-vocabulary (OOV) tokens — words that were not directly present in the FastText vocabulary list. However, FastText was still able to successfully generate embeddings for all 1,802 of these OOV tokens through its subword n-gram approach. This confirms FastText’s capability to handle rare and unseen words effectively, ensuring comprehensive token embedding coverage even for tokens outside its original training vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hgXm7bvAPfm3"
   },
   "source": [
    "#### 2. Word Similarity and Analogy Tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_check = ['protein', 'cell', 'gene', 'enzyme']\n",
    "\n",
    "for word in words_to_check:\n",
    "    print(f\"\\n Similar words to '{word}':\")\n",
    "    neighbors = fasttext_model.get_nearest_neighbors(word, k=5)\n",
    "    for score, neighbor in neighbors:\n",
    "        print(f\"{neighbor:20} Similarity Score: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import fasttext\n",
    "\n",
    "# Function to check if FastText can embed the word (non-zero vector)\n",
    "def can_fasttext_embed(word, model):\n",
    "    vec = fasttext_model.get_word_vector(word)\n",
    "    return not np.all(vec == 0)  # True if embedding is not all zeros\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_check = ['protein', 'cell', 'gene', 'enzyme']\n",
    "topn = 5  # Number of similar words to retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check embedding coverage for similar words\n",
    "total_similar_words = 0\n",
    "handled_similar_words = 0\n",
    "\n",
    "for word in words_to_check:\n",
    "    neighbors = fasttext_model.get_nearest_neighbors(word, k=topn)\n",
    "    print(f\"\\n Checking embedding coverage for similar words to '{word}':\")\n",
    "\n",
    "    for score, neighbor in neighbors:\n",
    "        total_similar_words += 1\n",
    "        if can_fasttext_embed(neighbor, fasttext_model):\n",
    "            handled_similar_words += 1\n",
    "            print(f\" {neighbor} (can embed)\")\n",
    "        else:\n",
    "            print(f\" {neighbor} (cannot embed)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Summary\n",
    "print(\"\\nSummary:\")\n",
    "print(f\"Total similar words suggested: {total_similar_words}\")\n",
    "print(f\"FastText could generate embeddings for: {handled_similar_words} out of {total_similar_words}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h7m3OKkVbh1G"
   },
   "source": [
    "The FastText embedding model was evaluated on 20 similar words retrieved through nearest neighbor search. The analysis showed that FastText successfully generated embeddings for all 20 suggested words, demonstrating its robust ability to handle word variations and related terms through subword information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N8A7Mwa45xUx"
   },
   "source": [
    "#### 3. Subword Information with FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qz1V6PRmAWig"
   },
   "source": [
    "FastText represents each word as a bag of character n-grams. This allows FastText to:\n",
    "- Embed rare or unseen biomedical terms\n",
    "- Generalize better in morphologically rich languages\n",
    "- Handle typos and word variants  \n",
    "\n",
    "The final embedding of a word is computed as the **sum of the vectors of its subwords**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This shows what subwords FastText breaks a word into\n",
    "\n",
    "word = \"neurodegeneration\"\n",
    "\n",
    "# Get subwords\n",
    "subwords, _ = fasttext_model.get_subwords(word)\n",
    "\n",
    "print(f\"Subwords (character n-grams) of '{word}':\")\n",
    "print(subwords)\n",
    "\n",
    "# Get word vector (composed from subwords)\n",
    "vector = fasttext_model.get_word_vector(word)\n",
    "\n",
    "print(f\"\\nEmbedding for '{word}':\")\n",
    "print(vector[:10])\n",
    "print(f\"Vector shape: {vector.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nOqN_X8dALch"
   },
   "source": [
    "#### **4. Generate Embedding Matrix from Padded Tokens**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MtE3Gr5QCa_G"
   },
   "source": [
    "Each token becomes a 300-dimensional vector, and each sentence becomes a matrix of shape [max_len, 300].\n",
    "\n",
    "All these matrices together become a big 3D tensor of shape:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ih9INr-EnceE"
   },
   "source": [
    "##### 4.1 Generate and Save FastText Embeddings and NER Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and Save FastText Embeddings and NER Labels\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def get_sentence_embedding(tokens, model, max_len=371, emb_dim=300):\n",
    "    embeddings = []\n",
    "    for token in tokens:\n",
    "        vec = model.get_word_vector(token.lower())\n",
    "        embeddings.append(vec)\n",
    "    while len(embeddings) < max_len:\n",
    "        embeddings.append(np.zeros(emb_dim))\n",
    "    return np.array(embeddings)\n",
    "\n",
    "train_embeddings = []\n",
    "for example in padded_dataset['train']:\n",
    "    tokens = example['tokens']\n",
    "    emb_matrix = get_sentence_embedding(tokens, fasttext_model)\n",
    "    train_embeddings.append(emb_matrix)\n",
    "\n",
    "train_embeddings = np.stack(train_embeddings)\n",
    "np.save('train_embeddings.npy', train_embeddings)\n",
    "\n",
    "# Labels\n",
    "label_tensors = [torch.tensor(example['ner_ids'], dtype=torch.long) for example in padded_dataset['train']]\n",
    "padded_labels = pad_sequence(label_tensors, batch_first=True, padding_value=-100)\n",
    "np.save('train_labels.npy', padded_labels.numpy())\n",
    "\n",
    "# Print outputs\n",
    "print(\"Train embeddings shape:\", train_embeddings.shape)\n",
    "print(\"Train labels shape:\", padded_labels.shape)\n",
    "print(\"Embeddings and labels saved.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IxV9TBX6nxnM"
   },
   "source": [
    "This means every sentence is represented as a 371-token sequence, where each token is a 300-dimensional FastText vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yz_i6WXfoAKc"
   },
   "source": [
    "##### 4.2 Load Saved Embeddings and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Saved Embeddings and Labels\n",
    "\n",
    "train_embeddings = np.load('train_embeddings.npy')\n",
    "train_labels = np.load('train_labels.npy')\n",
    "\n",
    "X_train = torch.tensor(train_embeddings, dtype=torch.float32)\n",
    "y_train = torch.tensor(train_labels, dtype=torch.long)\n",
    "\n",
    "X_train_rnn = X_train.clone()\n",
    "y_train_rnn = y_train.clone()\n",
    "X_train_bilstm = X_train.clone()\n",
    "y_train_bilstm = y_train.clone()\n",
    "\n",
    "# Print outputs\n",
    "print(\"Loaded embedding tensor shape:\", X_train.shape)\n",
    "print(\"Loaded label tensor shape:\", y_train.shape, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7G5hAGO0szbT"
   },
   "source": [
    "#### **5. Experiments with RNN and Bi-LSTM Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A1aIFiZmrN9A"
   },
   "source": [
    "#### Define Hyperparameters and Compute Class Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HIgCUMwxpvDc"
   },
   "source": [
    "##### Training Setup Configuration (Key Parameters)\n",
    "\n",
    "      - **Embedding dimension**: `input_dim = 300`  \n",
    "        → FastText vector size per token\n",
    "\n",
    "      - **RNN hidden size**: `hidden_dim_rnn = 256`  \n",
    "        → Uni-directional RNN output size\n",
    "\n",
    "      - **BiLSTM hidden size**: `hidden_dim_bilstm = 128`  \n",
    "        → 128 per direction → Final output = 128 × 2 = 256\n",
    "\n",
    "      - **Batch size**: `batch_size = 32`  \n",
    "        → Number of sequences per training step\n",
    "\n",
    "      - **Learning rate**: `learning_rate = 0.001`  \n",
    "        → Controls optimizer step size\n",
    "\n",
    "      - **Epochs**: `num_epochs = 5`  \n",
    "        → Max training iterations (early stopping may halt earlier)\n",
    "\n",
    "      - **Dropout**: `dropout_rate = 0.3`  \n",
    "        → Prevents overfitting by randomly deactivating neurons\n",
    "\n",
    "      - **Padding index**: `padding_idx = -100`  \n",
    "        → Ignores padding tokens in loss computation\n",
    "\n",
    "      - **Weighted loss**: `use_weighted_loss = True`  \n",
    "        → Handles class imbalance by emphasizing rare classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oDG9l7fQtsCk"
   },
   "source": [
    "##### 5.1 Define Hyperparameters and Compute Class Weights\n",
    "\n",
    "      Class Weights & Loss Setup (with Purpose)\n",
    "\n",
    "      - **Defined key hyperparameters** (input/output dimensions, hidden sizes, etc.).\n",
    "      - **Counted label frequencies** in the training data.\n",
    "      - **Computed class weights** using inverse frequency to handle label imbalance.\n",
    "      - **Used `CrossEntropyLoss` with weights** to:\n",
    "        - Prevent the model from over-predicting the majority class (`O`)\n",
    "        - Encourage learning from rare but important classes like `B-LF` and `B-AC`\n",
    "      - **Ignored padded tokens** using `ignore_index=-100` to focus training only on real tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Hyperparameters and Compute Class Weights ---\n",
    "\n",
    "from collections import Counter\n",
    "import torch.nn as nn\n",
    "\n",
    "input_dim = 300\n",
    "hidden_dim_rnn = 256\n",
    "hidden_dim_bilstm = 128\n",
    "output_dim = len(set(ner2id.values()))\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "num_epochs = 5\n",
    "dropout_rate = 0.3\n",
    "padding_idx = -100\n",
    "use_weighted_loss = True\n",
    "\n",
    "# Class weights\n",
    "all_labels_flat = [label for example in padded_dataset['train'] for label in example['ner_ids']]\n",
    "label_counts = Counter(all_labels_flat)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "total_labels = sum(label_counts.values())\n",
    "weights = [total_labels / (len(label_counts) * label_counts[i]) for i in range(output_dim)]\n",
    "weights = torch.tensor(weights, dtype=torch.float32).to(device)\n",
    "\n",
    "# Loss\n",
    "if use_weighted_loss:\n",
    "    loss_fn_rnn = nn.CrossEntropyLoss(weight=weights, ignore_index=padding_idx)\n",
    "    loss_fn_bilstm = nn.CrossEntropyLoss(weight=weights, ignore_index=padding_idx)\n",
    "else:\n",
    "    loss_fn_rnn = nn.CrossEntropyLoss(ignore_index=padding_idx)\n",
    "    loss_fn_bilstm = nn.CrossEntropyLoss(ignore_index=padding_idx)\n",
    "\n",
    "# Print outputs\n",
    "print(\"Label counts:\", dict(label_counts))\n",
    "print(\"Computed class weights:\")\n",
    "for i, w in enumerate(weights.tolist()):\n",
    "    print(f\"   Class {i}: Weight = {w:.4f}\")\n",
    "print(\"Loss functions initialized.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6OUb8PWSpC6Z"
   },
   "source": [
    "  1. Label Distribution:\n",
    "  - **Class 3 (O)** — *Non-entity tokens*: `62,474`  \n",
    "  - **Class 2 (I-LF)** — *Inside Long Form*: `9,525`  \n",
    "  - **Class 0 (B-AC)** — *Beginning of Abbreviation*: `6,626`  \n",
    "  - **Class 1 (B-LF)** — *Beginning of Long Form*: `3,923` (rarest)\n",
    "\n",
    "  This indicates that over **75%** of tokens belong to the `\"O\"` class, while the actual informative labels like `B-AC` and `B-LF` are relatively sparse.\n",
    "\n",
    "  2. Computed Class Weights (Inverse Frequency):\n",
    "  - **B-LF** (Class 1) gets the **highest weight** `5.26` to emphasize learning from rare but critical tokens.\n",
    "  - **B-AC** (Class 0) is also upweighted to `3.11`.\n",
    "  - **I-LF** (Class 2) has a moderate weight of `2.17`.\n",
    "  - **O** (Class 3), being very frequent, is **downweighted** to `0.33`.\n",
    "\n",
    "  These weights are passed into `CrossEntropyLoss` to **counter class imbalance** and help the model **focus on underrepresented but meaningful tags**.\n",
    "\n",
    "\n",
    "  3. Interpretation of Label Counts and Class Weights\n",
    "  The dataset shows a **significant class imbalance**, which is common in sequence labeling tasks like biomedical abbreviation detection:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fwf_P4CiyUG0"
   },
   "source": [
    "#### DataLoader for Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorDataset and DataLoaders\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_dataset_rnn = TensorDataset(X_train_rnn, y_train_rnn)\n",
    "train_dataset_bilstm = TensorDataset(X_train_bilstm, y_train_bilstm)\n",
    "\n",
    "train_loader_rnn = DataLoader(train_dataset_rnn, batch_size=batch_size, shuffle=True)\n",
    "train_loader_bilstm = DataLoader(train_dataset_bilstm, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Print outputs\n",
    "print(\"RNN batches per epoch:\", len(train_loader_rnn))\n",
    "print(\"BiLSTM batches per epoch:\", len(train_loader_bilstm), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HwzYuu00yYvN"
   },
   "source": [
    "The number of batches per epoch is computed by dividing the total number of examples by the batch size.\n",
    "\n",
    "In our case:\n",
    "\n",
    "Total examples = 2000 Batch size = 32 [ 2000/32 = 62.5]\n",
    "\n",
    "Thus, we have 63 batches per epoch (62 full batches + 1 smaller batch)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xw1QGMRKuYnu"
   },
   "source": [
    "#### 5.2 Define and Initialize Models\n",
    "\n",
    "    Defined two sequence models:\n",
    "\n",
    "    RNNTagger: A uni-directional RNN followed by dropout and a linear layer.\n",
    "\n",
    "    BiLSTMTagger: A bidirectional LSTM (BiLSTM) followed by dropout and a linear layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and Initialize Models\n",
    "\n",
    "class RNNTagger(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout_rate):\n",
    "        super(RNNTagger, self).__init__()\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, batch_first=True, bidirectional=False)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "class BiLSTMTagger(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout_rate):\n",
    "        super(BiLSTMTagger, self).__init__()\n",
    "        self.bilstm = nn.LSTM(input_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.bilstm(x)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Instantiate\n",
    "rnn_model = RNNTagger(input_dim, hidden_dim_rnn, output_dim, dropout_rate).to(device)\n",
    "bilstm_model = BiLSTMTagger(input_dim, hidden_dim_bilstm, output_dim, dropout_rate).to(device)\n",
    "\n",
    "# Print outputs\n",
    "print(\"RNN model initialized:\")\n",
    "print(rnn_model)\n",
    "print(\"\\n BiLSTM model initialized:\")\n",
    "print(bilstm_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E3CdZGhBrfU_"
   },
   "source": [
    "###### Model Initialization Summary\n",
    "\n",
    "Both models were successfully initialized with the following configurations:\n",
    "\n",
    "- **RNN Model (`RNNTagger`)**\n",
    "  - Input size: `300`\n",
    "  - Hidden size: `256` (uni-directional)\n",
    "  - Output layer: `Linear(256 → 4)`\n",
    "  - Dropout: `0.3`\n",
    "\n",
    "- **BiLSTM Model (`BiLSTMTagger`)**\n",
    "  - Input size: `300`\n",
    "  - Hidden size: `128` per direction → `256` total (bidirectional)\n",
    "  - Output layer: `Linear(256 → 4)`\n",
    "  - Dropout: `0.3`\n",
    "\n",
    "Both models produce output of shape `[batch_size, seq_len, 4]` for token-level classification across 4 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6iSpO88VrlvI"
   },
   "source": [
    "#### 5.3 Training Loop for RNN and BiLSTM Models\n",
    "\n",
    "      - Defined separate optimizers for RNN and BiLSTM using Adam.\n",
    "      - Created a shared `train_model()` function that:\n",
    "        - Sets model to training mode\n",
    "        - Loops through each epoch and batch\n",
    "        - Performs forward pass, computes loss, backpropagation, and optimizer step\n",
    "        - Logs average training loss per epoch\n",
    "      - Trained both models independently using their respective dataloaders and loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from torch import optim\n",
    "\n",
    "# Define optimizers for both models\n",
    "optimizer_rnn = optim.Adam(rnn_model.parameters(), lr=learning_rate)\n",
    "optimizer_bilstm = optim.Adam(bilstm_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Define training function (updated with F1)\n",
    "def train_model(model, train_loader, optimizer, criterion, num_epochs, model_name=\"Model\"):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    train_f1_scores = []\n",
    "\n",
    "    print(f\"\\nTraining started for {model_name}\\n\")\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        total_loss = 0\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "\n",
    "        for batch_embeddings, batch_labels in train_loader:\n",
    "            batch_embeddings = batch_embeddings.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_embeddings)\n",
    "            loss = criterion(outputs.view(-1, output_dim), batch_labels.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Collect predictions and true labels for F1 calculation\n",
    "            preds = torch.argmax(outputs, dim=-1)\n",
    "            for i in range(batch_labels.shape[0]):\n",
    "                for j in range(batch_labels.shape[1]):\n",
    "                    if batch_labels[i][j] != -100:\n",
    "                        y_true.append(batch_labels[i][j].item())\n",
    "                        y_pred.append(preds[i][j].item())\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        train_losses.append(avg_loss)\n",
    "\n",
    "        # Calculate F1-score for training\n",
    "        train_f1 = f1_score(y_true, y_pred, average='micro')\n",
    "        train_f1_scores.append(train_f1)\n",
    "\n",
    "        print(f\"{model_name} - Epoch {epoch}/{num_epochs} | Average Training Loss: {avg_loss:.4f} | Train F1: {train_f1:.4f}\")\n",
    "\n",
    "    print(f\"Training completed for {model_name}\\n\")\n",
    "    return train_losses, train_f1_scores\n",
    "\n",
    "# Train both models\n",
    "rnn_losses, rnn_train_f1 = train_model(rnn_model, train_loader_rnn, optimizer_rnn, loss_fn_rnn, num_epochs, model_name=\"RNN\")\n",
    "bilstm_losses, bilstm_train_f1 = train_model(bilstm_model, train_loader_bilstm, optimizer_bilstm, loss_fn_bilstm, num_epochs, model_name=\"BiLSTM\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e_bhLoG7rxEr"
   },
   "source": [
    "BiLSTM learns faster and generalizes better on the training set compared to RNN, achieving higher F1-scores with lower training loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4SSXSgwr0z_"
   },
   "source": [
    "#### 5.4 Compare Training Losses for RNN and BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Epochs\n",
    "epochs_rnn = range(1, len(rnn_losses) + 1)\n",
    "epochs_bilstm = range(1, len(bilstm_losses) + 1)\n",
    "\n",
    "# Create 1 row, 2 columns of subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# --- Plot Training Loss ---\n",
    "ax1.plot(epochs_rnn, rnn_losses, marker='o', linestyle='-', color='royalblue', label='RNN Loss')\n",
    "ax1.plot(epochs_bilstm, bilstm_losses, marker='s', linestyle='--', color='darkorange', label='BiLSTM Loss')\n",
    "ax1.set_title('Training Loss per Epoch')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Average Training Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# --- Plot Training F1 Accuracy ---\n",
    "ax2.plot(epochs_rnn, rnn_train_f1, marker='o', linestyle='-', color='green', label='RNN Train F1')\n",
    "ax2.plot(epochs_bilstm, bilstm_train_f1, marker='s', linestyle='--', color='red', label='BiLSTM Train F1')\n",
    "ax2.set_title('Training F1 Score per Epoch')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Training F1 Score')\n",
    "ax2.set_ylim(0, 1)  # F1 score is between 0 and 1\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "# Adjust layout\n",
    "plt.suptitle('Training Performance: RNN vs BiLSTM', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sHb-FALHahQT"
   },
   "source": [
    "## Evaluation on Validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Is5TF6Dar38f"
   },
   "source": [
    "##### Prepare Validation Embeddings and Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IiX0RfWAsPZE"
   },
   "source": [
    "Validation Setup Parameters:\n",
    "Number of Epochs: 50 epochs\n",
    "\n",
    "Early Stopping Patience: Configured to 5 epochs — training stops if there is no improvement in validation loss for 5 consecutive epochs.\n",
    "\n",
    "Batch Size: Fixed at 32 for both training and validation phases.\n",
    "\n",
    "Loss Function: Used * *italicized textCrossEntropyLoss with class weights** to handle class imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g4fZRShWvDZT"
   },
   "source": [
    "#### 5.5 Prepare Validation Embeddings and Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uO3cZLmTvik4"
   },
   "source": [
    "\n",
    "      - **Generated FastText embeddings** for each validation sentence and stacked them into a numpy array.\n",
    "      - **Saved embeddings** as `val_embeddings.npy` for reuse during evaluation.\n",
    "      - **Processed `ner_ids` labels** for validation:\n",
    "        - Padded each label sequence to a fixed length (`max_len = 371`) using `-100` as the ignore index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6tzDwlkBwDuC"
   },
   "source": [
    "##### Training + Validation Setup\n",
    "\n",
    "  - **Number of Epochs**: `50`  \n",
    "    → Maximum training duration for both RNN and BiLSTM\n",
    "\n",
    "  - **Early Stopping**:\n",
    "    - **Patience**: `5` epochs  \n",
    "      → Training stops if validation loss doesn't improve for 5 consecutive epochs\n",
    "    - **Model Checkpoint Paths**:\n",
    "      - `best_rnn.pth` for RNN\n",
    "      - `best_bilstm.pth` for BiLSTM\n",
    "\n",
    "  - **Loss Function**:  \n",
    "    - `CrossEntropyLoss` with:\n",
    "      - **Class weights** (to address label imbalance)\n",
    "      - `ignore_index = -100` (to skip padded tokens)\n",
    "\n",
    "  - **Monitoring Metric**:  \n",
    "    - **Validation Loss** (no early stopping on accuracy or F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Validation Embeddings and Labels\n",
    "\n",
    "# Create validation embeddings\n",
    "val_embeddings = []\n",
    "for example in padded_dataset['validation']:\n",
    "    tokens = example['tokens']\n",
    "    emb_matrix = get_sentence_embedding(tokens, fasttext_model)\n",
    "    val_embeddings.append(emb_matrix)\n",
    "\n",
    "val_embeddings = np.stack(val_embeddings)\n",
    "np.save('val_embeddings.npy', val_embeddings)\n",
    "print(\"Validation embeddings shape:\", val_embeddings.shape)\n",
    "\n",
    "# Pad validation labels to max_len\n",
    "max_len = 371\n",
    "val_label_tensors = []\n",
    "for example in padded_dataset['validation']:\n",
    "    ner_ids = example['ner_ids']\n",
    "    ner_ids_tensor = torch.tensor(ner_ids, dtype=torch.long)\n",
    "    if len(ner_ids_tensor) < max_len:\n",
    "        pad_size = max_len - len(ner_ids_tensor)\n",
    "        ner_ids_tensor = torch.cat([ner_ids_tensor, torch.full((pad_size,), -100, dtype=torch.long)])\n",
    "    else:\n",
    "        ner_ids_tensor = ner_ids_tensor[:max_len]\n",
    "    val_label_tensors.append(ner_ids_tensor)\n",
    "\n",
    "padded_val_labels = torch.stack(val_label_tensors)\n",
    "np.save('val_labels.npy', padded_val_labels.numpy())\n",
    "print(\" Validation labels shape:\", padded_val_labels.shape, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Validation Data ---\n",
    "\n",
    "val_embeddings = np.load('val_embeddings.npy')\n",
    "val_labels = np.load('val_labels.npy')\n",
    "\n",
    "val_embedding_tensor = torch.tensor(val_embeddings, dtype=torch.float).to(device)\n",
    "val_labels = torch.tensor(val_labels, dtype=torch.long).to(device)\n",
    "\n",
    "print(\"Loaded validation data:\")\n",
    "print(\"Embedding shape:\", val_embedding_tensor.shape)\n",
    "print(\"Label shape:\", val_labels.shape, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zyCwYs-swlrg"
   },
   "source": [
    "#### 5.6 Evaluation and EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluation and EarlyStopping ---\n",
    "\n",
    "def evaluate(model, val_embeddings, val_labels, criterion):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(val_embeddings)\n",
    "        val_loss = criterion(outputs.view(-1, output_dim), val_labels.view(-1))\n",
    "    return val_loss.item(), outputs\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, save_path='best_model.pth'):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.early_stop = False\n",
    "        self.save_path = save_path\n",
    "        self.best_epoch = 0\n",
    "\n",
    "    def __call__(self, val_loss, model, epoch):\n",
    "        if val_loss < self.best_loss:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_epoch = epoch\n",
    "            self.counter = 0\n",
    "            torch.save(model.state_dict(), self.save_path)\n",
    "            print(f\"Validation loss improved. Model saved to {self.save_path}\")\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            print(f\"No improvement for {self.counter} epoch(s).\")\n",
    "            if self.counter >= self.patience:\n",
    "                print(\"Early stopping triggered!\")\n",
    "                self.early_stop = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_validation(model, train_loader, val_embeddings, val_labels,\n",
    "                          optimizer, criterion, num_epochs, model_name,\n",
    "                          save_path, early_stopping):\n",
    "\n",
    "    train_losses, val_losses, val_f1_scores = [], [], []\n",
    "\n",
    "    print(f\"\\n Starting training with validation for {model_name}...\\n\")\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch_embeddings, batch_labels in train_loader:\n",
    "            batch_embeddings = batch_embeddings.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_embeddings)\n",
    "            loss = criterion(outputs.view(-1, output_dim), batch_labels.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Validation loss + F1-score calculation\n",
    "        val_loss, val_outputs = evaluate(model, val_embeddings.to(device), val_labels.to(device), criterion)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        val_preds = torch.argmax(val_outputs, dim=-1)\n",
    "\n",
    "        # Flatten true labels and predicted labels (ignoring -100 padding)\n",
    "        y_true, y_pred = [], []\n",
    "        for i in range(val_labels.shape[0]):\n",
    "            for j in range(val_labels.shape[1]):\n",
    "                if val_labels[i][j] != -100:\n",
    "                    y_true.append(val_labels[i][j].item())\n",
    "                    y_pred.append(val_preds[i][j].item())\n",
    "\n",
    "        val_f1 = f1_score(y_true, y_pred, average='micro')\n",
    "        val_f1_scores.append(val_f1)\n",
    "\n",
    "        print(f\"{model_name} - Epoch {epoch+1}/{num_epochs} | \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {val_loss:.4f} | Val F1: {val_f1:.4f}\")\n",
    "\n",
    "        early_stopping(val_loss, model, epoch+1)\n",
    "        if early_stopping.early_stop:\n",
    "            break\n",
    "\n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load(save_path))\n",
    "    print(f\"\\n Best {model_name} loaded from epoch {early_stopping.best_epoch}.\\n\")\n",
    "\n",
    "    return train_losses, val_losses, val_f1_scores, early_stopping.best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Execute Training with EarlyStopping ---\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "# Create independent early stopping instances\n",
    "early_stopping_rnn = EarlyStopping(patience=5, save_path='best_rnn.pth')\n",
    "early_stopping_bilstm = EarlyStopping(patience=5, save_path='best_bilstm.pth')\n",
    "\n",
    "# Train RNN\n",
    "rnn_train_losses, rnn_val_losses, rnn_val_f1, rnn_best_epoch = train_with_validation(\n",
    "    rnn_model, train_loader_rnn, val_embedding_tensor, val_labels,\n",
    "    optimizer_rnn, loss_fn_rnn, num_epochs,\n",
    "    model_name=\"RNN\", save_path=\"best_rnn.pth\", early_stopping=early_stopping_rnn\n",
    ")\n",
    "\n",
    "# Train BiLSTM\n",
    "bilstm_train_losses, bilstm_val_losses, bilstm_val_f1, bilstm_best_epoch = train_with_validation(\n",
    "    bilstm_model, train_loader_bilstm, val_embedding_tensor, val_labels,\n",
    "    optimizer_bilstm, loss_fn_bilstm, num_epochs,\n",
    "    model_name=\"BiLSTM\", save_path=\"best_bilstm.pth\", early_stopping=early_stopping_bilstm\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sSsUdKjN0VmM"
   },
   "source": [
    "#####  **Observations: RNN vs BiLSTM (with Early Stopping)**\n",
    "\n",
    "Early stopping was triggered for both RNN and BiLSTM models after no improvement for 5 consecutive epochs in validation loss.\n",
    "\n",
    "##### **For RNN:**\n",
    "\n",
    "Training stopped at epoch 6.\n",
    "\n",
    "Best model was found at epoch 1, with a validation F1-score of 73.73%.\n",
    "\n",
    "Even though training loss continued to decrease, validation loss did not consistently improve, indicating potential early overfitting.\n",
    "\n",
    "##### **For BiLSTM:**\n",
    "\n",
    "Training stopped at epoch 8.\n",
    "\n",
    "Best model was found at epoch 3, with a validation F1-score of 84.71%.\n",
    "\n",
    "BiLSTM consistently maintained lower validation loss and higher F1 scores compared to RNN across epochs.\n",
    "\n",
    "Overall, BiLSTM outperformed RNN in terms of both validation F1-score and stability across epochs, suggesting it generalized better on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Plot for RNN ---\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "epochs_rnn = list(range(1, len(rnn_train_losses) + 1))\n",
    "\n",
    "# RNN Loss Plot\n",
    "ax1.plot(epochs_rnn, rnn_train_losses, marker='o', linestyle='-', color='royalblue', label='Train Loss')\n",
    "ax1.plot(epochs_rnn, rnn_val_losses, marker='s', linestyle='--', color='goldenrod', label='Validation Loss')\n",
    "ax1.axvline(x=rnn_best_epoch, color='red', linestyle='--', label=f'Early Stop @ Epoch {rnn_best_epoch}')\n",
    "ax1.set_title('RNN: Loss Curves')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# RNN F1 Score Plot\n",
    "ax2.plot(epochs_rnn, rnn_val_f1, marker='^', linestyle='-', color='green', label='Validation F1')\n",
    "ax2.axvline(x=rnn_best_epoch, color='red', linestyle='--', label=f'Early Stop @ Epoch {rnn_best_epoch}')\n",
    "ax2.set_title('RNN: Validation F1 Curve')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('F1 Score')\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.suptitle('RNN Training Summary', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n",
    "\n",
    "# --- Plot for BiLSTM ---\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "epochs_bilstm = list(range(1, len(bilstm_train_losses) + 1))\n",
    "\n",
    "# BiLSTM Loss Plot\n",
    "ax1.plot(epochs_bilstm, bilstm_train_losses, marker='o', linestyle='-', color='navy', label='Train Loss')\n",
    "ax1.plot(epochs_bilstm, bilstm_val_losses, marker='s', linestyle='--', color='darkorange', label='Validation Loss')\n",
    "ax1.axvline(x=bilstm_best_epoch, color='red', linestyle='--', label=f'Early Stop @ Epoch {bilstm_best_epoch}')\n",
    "ax1.set_title('BiLSTM: Loss Curves')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# BiLSTM F1 Score Plot\n",
    "ax2.plot(epochs_bilstm, bilstm_val_f1, marker='^', linestyle='-', color='green', label='Validation F1')\n",
    "ax2.axvline(x=bilstm_best_epoch, color='red', linestyle='--', label=f'Early Stop @ Epoch {bilstm_best_epoch}')\n",
    "ax2.set_title('BiLSTM: Validation F1 Curve')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('F1 Score')\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.suptitle('BiLSTM Training Summary', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ubyY_Mv70gr0"
   },
   "source": [
    "\n",
    "#####  **Key Takeaways**\n",
    "1. Early stopping occurred at Epoch 1 for RNN and Epoch 3 for BiLSTM.\n",
    "\n",
    "2. Training loss decreased steadily for both models; however, validation loss fluctuated after the best epochs.\n",
    "\n",
    "3. BiLSTM achieved higher and more stable validation F1-scores compared to RNN.\n",
    "\n",
    "4. RNN showed signs of early overfitting, with validation F1 peaking early and then slightly dropping.\n",
    "\n",
    "5. BiLSTM generalized better, maintaining both lower validation loss and higher F1 accuracy across epochs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HGYExEMf5CRv"
   },
   "source": [
    "#### 6. RNN vs BiLSTM: Token Predictions and Sequence-Length Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M0XSOBlr5F4d"
   },
   "source": [
    "6.1 RNN vs BiLSTM: Token Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Extract validation tokens and labels from padded_dataset\n",
    "val_tokens = [example['tokens'] for example in padded_dataset['validation']]\n",
    "val_true = [example['ner_ids_padded'] for example in padded_dataset['validation']]\n",
    "\n",
    "# 2. Reload best models\n",
    "rnn_model.load_state_dict(torch.load('best_rnn.pth'))\n",
    "bilstm_model.load_state_dict(torch.load('best_bilstm.pth'))\n",
    "rnn_model.eval()\n",
    "bilstm_model.eval()\n",
    "\n",
    "# 3. Get predictions on validation embeddings\n",
    "with torch.no_grad():\n",
    "    rnn_outputs = rnn_model(val_embedding_tensor)\n",
    "    bilstm_outputs = bilstm_model(val_embedding_tensor)\n",
    "\n",
    "    rnn_preds = torch.argmax(rnn_outputs, dim=-1).cpu()\n",
    "    bilstm_preds = torch.argmax(bilstm_outputs, dim=-1).cpu()\n",
    "\n",
    "# 4. Restore id2ner mapping\n",
    "id2ner = {v: k for k, v in ner2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_prediction_errors(val_tokens, val_true, rnn_preds, bilstm_preds, id2ner, num_examples=3):\n",
    "    print(\"Showing only tokens where RNN or BiLSTM mispredicted:\\n\")\n",
    "    shown = 0\n",
    "\n",
    "    for i in range(len(val_tokens)):\n",
    "        has_error = False\n",
    "        error_lines = []\n",
    "\n",
    "        for j, token in enumerate(val_tokens[i]):\n",
    "            if val_true[i][j] == -100:\n",
    "                continue  # skip padding\n",
    "\n",
    "            true_label = id2ner[val_true[i][j]]\n",
    "            rnn_label = id2ner[rnn_preds[i][j].item()]\n",
    "            bilstm_label = id2ner[bilstm_preds[i][j].item()]\n",
    "\n",
    "            if true_label != rnn_label or true_label != bilstm_label:\n",
    "                has_error = True\n",
    "                error_lines.append(f\"{token:15} | True: {true_label:6} | RNN: {rnn_label:6} | BiLSTM: {bilstm_label:6}\")\n",
    "\n",
    "        if has_error:\n",
    "            print(f\"\\n Example {shown+1} (Sentence {i}) -------------------------------\")\n",
    "            for line in error_lines:\n",
    "                print(line)\n",
    "            shown += 1\n",
    "\n",
    "        if shown == num_examples:\n",
    "            break\n",
    "\n",
    "show_prediction_errors(val_tokens, val_true, rnn_preds, bilstm_preds, id2ner, num_examples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BJSGIphW3-f7"
   },
   "source": [
    "\n",
    "### **Key Observations from Errors**\n",
    "\n",
    "1. **RNN makes more mistakes on normal words**  \n",
    "   → It wrongly labels words like \"weight\", \"amount\", \"ground\" as entities.\n",
    "\n",
    "2. **BiLSTM handles long phrases better**  \n",
    "   → It correctly detects full long-form sequences, while RNN often misses or breaks them.\n",
    "\n",
    "3. **RNN gets confused by punctuation and symbols**  \n",
    "   → Labels things like `-` or `,` as entities. BiLSTM mostly avoids this.\n",
    "\n",
    "4. **BiLSTM predicts abbreviations more accurately**  \n",
    "   → For tokens like `NUpE`, `gdh`, `recP`, BiLSTM gives correct tags, RNN sometimes doesn't.\n",
    "\n",
    "5. **BiLSTM is more consistent**  \n",
    "   → It gives fewer random or broken predictions across similar patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_token_accuracy(val_true, rnn_preds, bilstm_preds):\n",
    "    total_tokens = 0\n",
    "    rnn_correct = 0\n",
    "    bilstm_correct = 0\n",
    "\n",
    "    for i in range(len(val_true)):\n",
    "        for j in range(len(val_true[i])):\n",
    "            true_id = val_true[i][j]\n",
    "            if true_id == -100:\n",
    "                continue  # ignore padding\n",
    "\n",
    "            total_tokens += 1\n",
    "            if rnn_preds[i][j].item() == true_id:\n",
    "                rnn_correct += 1\n",
    "            if bilstm_preds[i][j].item() == true_id:\n",
    "                bilstm_correct += 1\n",
    "\n",
    "    rnn_accuracy = (rnn_correct / total_tokens) * 100\n",
    "    bilstm_accuracy = (bilstm_correct / total_tokens) * 100\n",
    "\n",
    "    rnn_error = 100 - rnn_accuracy\n",
    "    bilstm_error = 100 - bilstm_accuracy\n",
    "\n",
    "    print(f\" Token-Level Accuracy:\")\n",
    "    print(f\"RNN:     {rnn_accuracy:.2f}% ({rnn_correct}/{total_tokens})\")\n",
    "    print(f\"BiLSTM:  {bilstm_accuracy:.2f}% ({bilstm_correct}/{total_tokens})\")\n",
    "    print(f\"RNN Error Rate:    {rnn_error:.2f}%\")\n",
    "    print(f\"BiLSTM Error Rate: {bilstm_error:.2f}%\")\n",
    "\n",
    "# Call it\n",
    "compute_token_accuracy(val_true, rnn_preds, bilstm_preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bFRVrccF5lar"
   },
   "source": [
    "#####  **Token-Level Accuracy: Summary**\n",
    "\n",
    "1. **BiLSTM is significantly more accurate than RNN**\n",
    "   - **BiLSTM Accuracy**: 84.71%\n",
    "   - **RNN Accuracy**: 73.73%\n",
    "\n",
    "2. **RNN makes more mistakes**\n",
    "   - **RNN Error Rate**: 26.27%\n",
    "   - **BiLSTM Error Rate**: 15.29%\n",
    "\n",
    "3. **BiLSTM correctly predicted ~659 more tokens**  \n",
    "   → Out of 6004 tokens, BiLSTM got **659 more right** than RNN.\n",
    "\n",
    "##### Takeaway:\n",
    "> BiLSTM is clearly better at token-level classification — more accurate, more consistent, and makes fewer errors than RNN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['RNN', 'BiLSTM']\n",
    "accuracies = [73.73, 84.71]\n",
    "errors = [26.27, 15.29]\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.bar(labels, accuracies, color=['royalblue', 'darkorange'])\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.ylim(0, 100)\n",
    "plt.title('Token-Level Accuracy Comparison')\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9aGzFHh5cJP"
   },
   "source": [
    "6.2 RNN vs BiLSTM  Sequence-Length Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def compute_f1_by_length(val_tokens, val_true, rnn_preds, bilstm_preds):\n",
    "    # Define bins\n",
    "    bins = {\n",
    "        \"Short (<=100)\": [],\n",
    "        \"Medium (101–200)\": [],\n",
    "        \"Long (>200)\": []\n",
    "    }\n",
    "\n",
    "    for i, tokens in enumerate(val_tokens):\n",
    "        seq_len = sum([1 for t in val_true[i] if t != -100])  # true token length\n",
    "\n",
    "        if seq_len <= 100:\n",
    "            bins[\"Short (<=100)\"].append(i)\n",
    "        elif seq_len <= 200:\n",
    "            bins[\"Medium (101–200)\"].append(i)\n",
    "        else:\n",
    "            bins[\"Long (>200)\"].append(i)\n",
    "\n",
    "    print(\"Length-wise F1 Comparison (Micro-Averaged):\\n\")\n",
    "    for bin_name, indices in bins.items():\n",
    "        rnn_true, rnn_pred = [], []\n",
    "        bilstm_true, bilstm_pred = [], []\n",
    "\n",
    "        for i in indices:\n",
    "            for j in range(len(val_true[i])):\n",
    "                if val_true[i][j] == -100:\n",
    "                    continue\n",
    "                rnn_true.append(val_true[i][j])\n",
    "                rnn_pred.append(rnn_preds[i][j].item())\n",
    "                bilstm_true.append(val_true[i][j])\n",
    "                bilstm_pred.append(bilstm_preds[i][j].item())\n",
    "\n",
    "        if rnn_true:\n",
    "            rnn_f1 = f1_score(rnn_true, rnn_pred, average='micro')\n",
    "            bilstm_f1 = f1_score(bilstm_true, bilstm_pred, average='micro')\n",
    "            print(f\"{bin_name:18} | RNN F1: {rnn_f1:.4f} | BiLSTM F1: {bilstm_f1:.4f} | Samples: {len(indices)}\")\n",
    "        else:\n",
    "            print(f\"{bin_name:18} | No samples\")\n",
    "\n",
    "# Call the function\n",
    "compute_f1_by_length(val_tokens, val_true, rnn_preds, bilstm_preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DBIwtDn_4atp"
   },
   "source": [
    "### **Length-wise F1 Comparison**\n",
    "\n",
    "1. **BiLSTM performs better across all lengths**\n",
    "   - Especially strong improvement on **short sequences**.\n",
    "\n",
    "2. **Short sequences (≤100 tokens)**\n",
    "   - **RNN F1**: 74.38%\n",
    "   - **BiLSTM F1**: 85.53%  \n",
    "   → BiLSTM is ~11% better.\n",
    "\n",
    "3. **Medium sequences (101–200 tokens)**\n",
    "   - **RNN F1**: 69.75%\n",
    "   - **BiLSTM F1**: 79.59%  \n",
    "   → BiLSTM leads by ~10%.\n",
    "\n",
    "4. **No long sequences (>200 tokens)**  \n",
    "   → None were present in the validation set.\n",
    "\n",
    "> BiLSTM outperforms RNN consistently across different sentence lengths, especially on short and medium-length inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "length_bins = ['Short (<=100)', 'Medium (101–200)']\n",
    "rnn_f1 = [0.7438, 0.6975]\n",
    "bilstm_f1 = [0.8553, 0.7959]\n",
    "\n",
    "x = range(len(length_bins))\n",
    "width = 0.35\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar([p - width/2 for p in x], rnn_f1, width=width, label='RNN', color='skyblue')\n",
    "plt.bar([p + width/2 for p in x], bilstm_f1, width=width, label='BiLSTM', color='orange')\n",
    "\n",
    "plt.xticks(ticks=x, labels=length_bins)\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.ylim(0, 1)\n",
    "plt.title(\"Length-wise F1 Score Comparison\")\n",
    "plt.legend()\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HhiFk0kHaWLL"
   },
   "source": [
    "## Evaluation on Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cMS2EmcfbSWN"
   },
   "source": [
    "#### 1. Load Test Embeddings and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating test embeddings using FastText\n",
    "test_embeddings = []\n",
    "test_label_tensors = []\n",
    "\n",
    "for example in padded_dataset['test']:\n",
    "    tokens = example['tokens']\n",
    "    ner_ids = example['ner_ids']\n",
    "\n",
    "    emb_matrix = get_sentence_embedding(tokens, fasttext_model)\n",
    "    test_embeddings.append(emb_matrix)\n",
    "\n",
    "    # Pad NER labels to max_len = 371\n",
    "    label_tensor = torch.tensor(ner_ids, dtype=torch.long)\n",
    "    pad_len = 371 - len(label_tensor)\n",
    "    if pad_len > 0:\n",
    "        label_tensor = torch.cat([label_tensor, torch.full((pad_len,), -100, dtype=torch.long)])\n",
    "    test_label_tensors.append(label_tensor)\n",
    "\n",
    "# Converting to arrays and save\n",
    "test_embeddings = np.stack(test_embeddings)\n",
    "test_labels = torch.stack(test_label_tensors)\n",
    "\n",
    "np.save('test_embeddings.npy', test_embeddings)\n",
    "np.save('test_labels.npy', test_labels.numpy())\n",
    "\n",
    "print(\"Test embeddings and labels saved:\")\n",
    "print(\"Embedding shape:\", test_embeddings.shape)\n",
    "print(\"Label shape:\", test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embeddings = np.load('test_embeddings.npy')\n",
    "test_labels = np.load('test_labels.npy')\n",
    "\n",
    "# Convert to tensors\n",
    "test_embedding_tensor = torch.tensor(test_embeddings, dtype=torch.float32).to(device)\n",
    "test_label_tensor = torch.tensor(test_labels, dtype=torch.long).to(device)\n",
    "\n",
    "print(\"Test Embedding Shape:\", test_embedding_tensor.shape)\n",
    "print(\"Test Label Shape:\", test_label_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ziM9FRcbY3-"
   },
   "source": [
    "#### 2. Load Best Saved Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best models\n",
    "rnn_model.load_state_dict(torch.load(\"best_rnn.pth\"))\n",
    "bilstm_model.load_state_dict(torch.load(\"best_bilstm.pth\"))\n",
    "\n",
    "rnn_model.eval()\n",
    "bilstm_model.eval()\n",
    "\n",
    "print(\"Loaded best RNN and BiLSTM models for testing.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o9wXMCovbdwV"
   },
   "source": [
    "#### 3. Predict on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, embeddings):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(embeddings)\n",
    "        preds = torch.argmax(outputs, dim=-1)\n",
    "    return preds\n",
    "\n",
    "rnn_test_preds = get_predictions(rnn_model, test_embedding_tensor)\n",
    "bilstm_test_preds = get_predictions(bilstm_model, test_embedding_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O5v1jOFVc3fa"
   },
   "source": [
    "#### 4. Align Predictions and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels_list = []\n",
    "rnn_preds_list = []\n",
    "bilstm_preds_list = []\n",
    "\n",
    "for i in range(test_label_tensor.shape[0]):\n",
    "    true_seq = test_label_tensor[i]\n",
    "    rnn_seq = rnn_test_preds[i]\n",
    "    bilstm_seq = bilstm_test_preds[i]\n",
    "\n",
    "    mask = true_seq != -100\n",
    "    seq_len = mask.sum().item()\n",
    "\n",
    "    # Convert IDs to NER labels\n",
    "    true_labels = [id2ner[true_seq[j].item()] for j in range(seq_len)]\n",
    "    rnn_labels = [id2ner[rnn_seq[j].item()] for j in range(seq_len)]\n",
    "    bilstm_labels = [id2ner[bilstm_seq[j].item()] for j in range(seq_len)]\n",
    "\n",
    "    true_labels_list.append(true_labels)\n",
    "    rnn_preds_list.append(rnn_labels)\n",
    "    bilstm_preds_list.append(bilstm_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8oIMKJBc-z6"
   },
   "source": [
    "#### 5. Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import classification_report\n",
    "\n",
    "print(\"RNN Test Classification Report:\")\n",
    "print(classification_report(true_labels_list, rnn_preds_list))\n",
    "\n",
    "print(\"\\nBiLSTM Test Classification Report:\")\n",
    "print(classification_report(true_labels_list, bilstm_preds_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hsaqztkskqYB"
   },
   "source": [
    "### Observation:\n",
    "\n",
    "- **Overall Performance**:  \n",
    "  BiLSTM achieved a micro-averaged F1-score of 0.69, significantly outperforming RNN's 0.49, indicating stronger prediction capability.\n",
    "\n",
    "- **Macro-Averaged Scores**:  \n",
    "  The macro F1-score improved from 0.50 (RNN) to 0.68 (BiLSTM), suggesting BiLSTM performed more consistently across all classes.\n",
    "\n",
    "- **Class-wise Comparison**:  \n",
    "  - **AC (Abbreviation)**:  \n",
    "    - RNN F1-score: **0.71**  \n",
    "    - BiLSTM F1-score: **0.76**\n",
    "  - **LF (Long Form)**:  \n",
    "    - RNN F1-score: **0.29**  \n",
    "    - BiLSTM F1-score: **0.59**\n",
    "\n",
    "- **Precision & Recall**:  \n",
    "  BiLSTM showed higher precision and recall across both classes, demonstrating better generalization and fewer false predictions.\n",
    "\n",
    "- **Error Reduction**:  \n",
    "  BiLSTM reduced entity fragmentation and improved boundary detection for multi-token entities, especially for long-form terms.\n",
    "\n",
    "- **Conclusion**:  \n",
    "  **BiLSTM outperforms RNN** in every metric on the test set, proving to be more suitable for biomedical NER tasks with its ability to capture bidirectional context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DYObQtCodFyC"
   },
   "source": [
    "#### 6. Token-Level Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_accuracy(true, pred):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for t_seq, p_seq in zip(true, pred):\n",
    "        for t, p in zip(t_seq, p_seq):\n",
    "            if t == p:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "    return correct / total * 100\n",
    "\n",
    "rnn_acc = token_accuracy(true_labels_list, rnn_preds_list)\n",
    "bilstm_acc = token_accuracy(true_labels_list, bilstm_preds_list)\n",
    "\n",
    "print(f\"Token Accuracy - RNN:    {rnn_acc:.2f}%\")\n",
    "print(f\"Token Accuracy - BiLSTM: {bilstm_acc:.2f}%\")\n",
    "\n",
    "models = ['RNN', 'BiLSTM']\n",
    "accuracies = [74.2, 85.1]  # example values\n",
    "errors = [25.8, 14.9]\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.bar(models, accuracies, color=['steelblue', 'darkorange'])\n",
    "plt.ylabel('Token-Level Accuracy (%)')\n",
    "plt.ylim(0, 100)\n",
    "plt.title('Token-Level Accuracy: RNN vs BiLSTM (Test Set)')\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WrIzHgn2lYjT"
   },
   "source": [
    "**Observation**:  \n",
    "  The higher token-level accuracy reaffirms that **BiLSTM is more precise and reliable** for token classification tasks in biomedical NER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Class labels\n",
    "labels = ['AC', 'LF']\n",
    "\n",
    "# F1-scores for each model\n",
    "rnn_f1 = [0.71, 0.29]\n",
    "bilstm_f1 = [0.76, 0.59]\n",
    "\n",
    "x = range(len(labels))\n",
    "width = 0.35\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.bar([p - width/2 for p in x], rnn_f1, width=width, label='RNN', color='royalblue')\n",
    "plt.bar([p + width/2 for p in x], bilstm_f1, width=width, label='BiLSTM', color='darkorange')\n",
    "\n",
    "plt.xticks(ticks=x, labels=labels)\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.ylim(0, 1)\n",
    "plt.title(\"Test Set: F1 Score per Class (RNN vs BiLSTM)\")\n",
    "plt.legend()\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jc-3dO21sd6X"
   },
   "source": [
    "#### Observation:\n",
    "- **BiLSTM** outperforms **RNN** on both classes.\n",
    "- The improvement is particularly significant for the `LF` (Long Form) class, where BiLSTM achieved **0.59** vs RNN's **0.29**.\n",
    "- This reinforces the superior ability of BiLSTM in capturing sequential dependencies and modeling complex token relationships.\n",
    "\n",
    "The plot provides a clear, visual comparison of how each model performs on individual entity types, making the case for BiLSTM’s better generalization in biomedical NER tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def extract_true_pred_labels(test_labels_tensor, model_preds_tensor):\n",
    "    y_true, y_pred = [], []\n",
    "    for true_seq, pred_seq in zip(test_labels_tensor, model_preds_tensor):\n",
    "        for true_id, pred_id in zip(true_seq, pred_seq):\n",
    "            if true_id.item() == -100:\n",
    "                continue\n",
    "            y_true.append(id2ner[true_id.item()])\n",
    "            y_pred.append(id2ner[pred_id.item()])\n",
    "    return y_true, y_pred\n",
    "\n",
    "# Get true/pred labels\n",
    "rnn_y_true, rnn_y_pred = extract_true_pred_labels(test_labels, rnn_test_preds)\n",
    "bilstm_y_true, bilstm_y_pred = extract_true_pred_labels(test_labels, bilstm_test_preds)\n",
    "\n",
    "# Generating confusion matrices\n",
    "labels = list(id2ner.values())\n",
    "cm_rnn = confusion_matrix(rnn_y_true, rnn_y_pred, labels=labels)\n",
    "cm_bilstm = confusion_matrix(bilstm_y_true, bilstm_y_pred, labels=labels)\n",
    "\n",
    "# Plot\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "sns.heatmap(cm_rnn, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels, ax=axs[0])\n",
    "axs[0].set_title(\"RNN Confusion Matrix\")\n",
    "axs[0].set_xlabel(\"Predicted\")\n",
    "axs[0].set_ylabel(\"True\")\n",
    "\n",
    "sns.heatmap(cm_bilstm, annot=True, fmt='d', cmap='Oranges', xticklabels=labels, yticklabels=labels, ax=axs[1])\n",
    "axs[1].set_title(\"BiLSTM Confusion Matrix\")\n",
    "axs[1].set_xlabel(\"Predicted\")\n",
    "axs[1].set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UeF1kQsmq7ht"
   },
   "source": [
    "#### Confusion Matrix Observations: RNN vs BiLSTM\n",
    "\n",
    "1. **BiLSTM outperforms RNN across all NER classes.**  \n",
    "   The BiLSTM model achieves higher true positives for all entity classes (`B-AC`, `B-LF`, `I-LF`) compared to the RNN model.\n",
    "\n",
    "2. **RNN shows significant confusion between entity types.**  \n",
    "   - `I-LF` is frequently misclassified as `O`, `B-LF`, and `B-AC` in RNN.\n",
    "   - This indicates RNN struggles to maintain correct label continuity within entities.\n",
    "\n",
    "3. **BiLSTM captures long-form entities better.**  \n",
    "   - Correct `I-LF` predictions: **1133 (BiLSTM)** vs **982 (RNN)**  \n",
    "   - BiLSTM more accurately models dependencies within multi-token entities.\n",
    "\n",
    "4. **RNN produces more false positives.**  \n",
    "   - RNN often misclassifies non-entity (`O`) tokens as entity labels (`B-AC`, `B-LF`, `I-LF`), leading to inflated false positives.\n",
    "\n",
    "5. **BiLSTM handles non-entity tokens (`O`) with higher accuracy.**  \n",
    "   - BiLSTM correctly predicts `O` for **6620** tokens compared to **5450** by RNN, reducing noise in entity detection.\n",
    "\n",
    "6. **Diagonal dominance is stronger in BiLSTM.**  \n",
    "   - The BiLSTM matrix shows clearer concentration along the diagonal, indicating better overall precision and reduced label confusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ZdI6tYNlleK"
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0AQGlYDBl-EU"
   },
   "source": [
    "1. **Task Overview**:  \n",
    "   The goal was to perform Named Entity Recognition (NER) on biomedical texts, identifying abbreviations (AC) and their corresponding long forms (LF).\n",
    "\n",
    "2. **Approach Summary**:  \n",
    "   - Used pre-trained FastText embeddings to represent subword-level semantics.  \n",
    "   - Implemented and trained both RNN and BiLSTM models for sequence labeling.  \n",
    "   - Applied class-weighted loss and early stopping based on validation loss to prevent overfitting.\n",
    "\n",
    "3. **Performance Highlights**:  \n",
    "   - **Validation Accuracy**: BiLSTM consistently outperformed RNN across all sequence lengths.  \n",
    "   - **Test Accuracy**:  \n",
    "     - **RNN Token Accuracy**: 73.41%  \n",
    "     - **BiLSTM Token Accuracy**: 86.92%  \n",
    "     - BiLSTM predicted ~13.5% more tokens correctly than RNN.  \n",
    "   - **F1 Scores** on Test Set:  \n",
    "     - RNN (Micro F1): 0.49  \n",
    "     - BiLSTM (Micro F1): 0.69  \n",
    "     - BiLSTM achieved much better balance between precision and recall.\n",
    "\n",
    "4. **Model Behavior**:  \n",
    "   - RNN often misclassified normal or punctuation tokens as entities.  \n",
    "   - BiLSTM demonstrated greater consistency and contextual understanding, especially for long-form (LF) entities.\n",
    "\n",
    "5. **Final Insight**:  \n",
    "   The experiment clearly shows that BiLSTM, with bidirectional context and subword-aware embeddings, is significantly more effective for biomedical NER compared to a simple RNN. It delivers higher accuracy, better generalization, and fewer false positives."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
